{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f531ddd",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction\n",
    "This notebook demonstrates how to predict customer churn using advanced classification algorithms such as Logistic Regression and Random Forest. We will walk through data loading, preprocessing, feature engineering, model training, evaluation, and extracting actionable business insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbf732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.3-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.3-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.6-cp312-cp312-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.6-cp312-cp312-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (111 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.4-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.4-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.2-cp312-cp312-macosx_10_13_x86_64.whl (11.6 MB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-macosx_14_0_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.2-cp312-cp312-macosx_10_13_x86_64.whl (11.6 MB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-macosx_14_0_x86_64.whl (6.6 MB)\n",
      "Using cached matplotlib-3.10.6-cp312-cp312-macosx_10_13_x86_64.whl (8.3 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl (9.3 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-macosx_10_13_x86_64.whl (293 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.0-cp312-cp312-macosx_10_13_x86_64.whl (2.4 MB)\n",
      "Using cached matplotlib-3.10.6-cp312-cp312-macosx_10_13_x86_64.whl (8.3 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-macosx_10_13_x86_64.whl (9.3 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-macosx_10_13_x86_64.whl (293 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.0-cp312-cp312-macosx_10_13_x86_64.whl (2.4 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-macosx_10_13_x86_64.whl (66 kB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-macosx_10_13_x86_64.whl (5.3 MB)\n",
      "Using cached pyparsing-3.2.4-py3-none-any.whl (113 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-macosx_14_0_x86_64.whl (23.6 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-macosx_10_13_x86_64.whl (66 kB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-macosx_10_13_x86_64.whl (5.3 MB)\n",
      "Using cached pyparsing-3.2.4-py3-none-any.whl (113 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-macosx_14_0_x86_64.whl (23.6 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, scipy, pandas, contourpy, scikit-learn, matplotlib, seaborn\n",
      "\u001b[?25lInstalling collected packages: pytz, tzdata, threadpoolctl, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, scipy, pandas, contourpy, scikit-learn, matplotlib, seaborn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [seaborn]5/16\u001b[0m [seaborn]ib]n]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 numpy-2.3.3 pandas-2.3.2 pillow-11.3.0 pyparsing-3.2.4 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.2 seaborn-0.13.2 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [seaborn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 numpy-2.3.3 pandas-2.3.2 pillow-11.3.0 pyparsing-3.2.4 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.2 seaborn-0.13.2 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages if needed\n",
    "%pip install pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08eacc",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "We will load the customer churn dataset and perform initial exploration to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (replace 'customer_churn.csv' with your actual file path)\n",
    "df = pd.read_csv('customer_churn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "df.info()\n",
    "df.describe()\n",
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4336849",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We will handle missing values, encode categorical variables, and scale features as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Example\n",
    "df = df.dropna()  # Drop missing values (customize as needed)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols = [col for col in categorical_cols if col != 'Churn']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "scaler = StandardScaler()\n",
    "feature_cols = [col for col in df.columns if col != 'Churn']\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f0f3f",
   "metadata": {},
   "source": [
    "## Feature Selection/Engineering\n",
    "Select relevant features for modeling. You can also create new features if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70281047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection and train/test split\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)  # Adjust if Churn is already numeric\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c4dd1",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We will train both Logistic Regression and Random Forest classifiers on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b210a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d998301",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate both models using classification metrics and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print('Logistic Regression Classification Report:')\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Evaluate Random Forest\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print('Random Forest Classification Report:')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0470068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key drivers of churn\n",
    "importances = rf.feature_importances_\n",
    "features = X.columns\n",
    "feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=feat_imp[:10], y=feat_imp.index[:10])\n",
    "plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression coefficients\n",
    "coefs = pd.Series(logreg.coef_[0], index=features).sort_values(key=abs, ascending=False)\n",
    "print('Top Logistic Regression Coefficients:')\n",
    "print(coefs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434fd47",
   "metadata": {},
   "source": [
    "## Actionable Business Insights\n",
    "Based on the model results and feature importances, summarize key drivers of churn and suggest business actions to reduce churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1ba9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrated how to use Logistic Regression and Random Forest to predict customer churn, identify key drivers, and generate actionable business intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742b86d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Let's explore the data visually to better understand churn distribution, feature relationships, and potential drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn distribution\n",
    "sns.countplot(x='Churn', data=df)\n",
    "plt.title('Churn Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of numerical features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df[num_cols].hist(figsize=(15,10), bins=20)\n",
    "plt.suptitle('Numerical Feature Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86721a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Churn rate by categorical features (example: gender, contract type)\n",
    "categorical_cols = [col for col in df.columns if 'Churn' not in col and df[col].dtype == 'uint8']\n",
    "for col in categorical_cols[:3]:  # Show for first 3 dummy variables as example\n",
    "    churn_rate = df.groupby(col)['Churn'].mean()\n",
    "    churn_rate.plot(kind='bar')\n",
    "    plt.title(f'Churn Rate by {col}')\n",
    "    plt.ylabel('Churn Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b878dfc",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering and Selection\n",
    "We will create new features, use domain knowledge, and apply feature selection techniques to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create tenure group feature if 'tenure' exists\n",
    "if 'tenure' in df.columns:\n",
    "    df['tenure_group'] = pd.cut(df['tenure'], bins=[0, 12, 24, 48, 60, np.inf], labels=['0-12','12-24','24-48','48-60','60+'])\n",
    "    df = pd.get_dummies(df, columns=['tenure_group'], drop_first=True)\n",
    "\n",
    "# Feature selection using RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "selector = RFE(LogisticRegression(max_iter=1000), n_features_to_select=10)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "selected_features = X_train.columns[selector.support_]\n",
    "print('Selected Features:', selected_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879e2d4",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "We will use GridSearchCV to find the best hyperparameters for both Logistic Regression and Random Forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd33d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression hyperparameter tuning\n",
    "logreg_params = {'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(max_iter=1000), logreg_params, cv=5, scoring='accuracy')\n",
    "logreg_grid.fit(X_train, y_train)\n",
    "print('Best Logistic Regression Params:', logreg_grid.best_params_)\n",
    "\n",
    "# Random Forest hyperparameter tuning\n",
    "rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5]}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print('Best Random Forest Params:', rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6eb7e",
   "metadata": {},
   "source": [
    "## Model Validation with K-Fold Cross-Validation\n",
    "We will use k-fold cross-validation to assess the robustness of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bc207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# K-Fold CV for best Logistic Regression\n",
    "logreg_cv_scores = cross_val_score(logreg_grid.best_estimator_, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Logistic Regression CV Accuracy:', logreg_cv_scores.mean())\n",
    "\n",
    "# K-Fold CV for best Random Forest\n",
    "rf_cv_scores = cross_val_score(rf_grid.best_estimator_, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print('Random Forest CV Accuracy:', rf_cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db10f19",
   "metadata": {},
   "source": [
    "## Model Explainability with SHAP\n",
    "We will use SHAP to interpret the predictions and understand feature contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc49ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP if not already installed\n",
    "import sys\n",
    "!{sys.executable} -m pip install shap\n",
    "\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(rf_grid.best_estimator_)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[1], X_test, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71dfe2",
   "metadata": {},
   "source": [
    "## Model Performance Visualization\n",
    "Visualize ROC curves and confusion matrices for both models to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbed063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ROC Curve for both models\n",
    "y_score_logreg = logreg_grid.best_estimator_.predict_proba(X_test)[:,1]\n",
    "y_score_rf = rf_grid.best_estimator_.predict_proba(X_test)[:,1]\n",
    "fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_score_logreg)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_score_rf)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_logreg, tpr_logreg, label='Logistic Regression')\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, logreg_grid.best_estimator_.predict(X_test))).plot(ax=axes[0], colorbar=False)\n",
    "axes[0].set_title('Logistic Regression')\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, rf_grid.best_estimator_.predict(X_test))).plot(ax=axes[1], colorbar=False)\n",
    "axes[1].set_title('Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6dd7e9",
   "metadata": {},
   "source": [
    "## Requirements and Environment Setup\n",
    "To reproduce this notebook, install the following packages: pandas, numpy, matplotlib, seaborn, scikit-learn, shap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements (uncomment if running in a new environment)\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cc49e",
   "metadata": {},
   "source": [
    "## Business Recommendations and Impact\n",
    "Based on the analysis, target the top drivers of churn with specific retention strategies. Quantify the potential impact by estimating how reducing churn in key segments could improve revenue or customer lifetime value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
